---
description: 
globs: 
alwaysApply: false
---
Dia AI Assistant: Technical Development Plan
Version: 1.0
Objective: This document outlines a structured development plan for building the Dia AI assistant. It is intended for a developer and breaks the project into distinct, actionable milestones.

1. Project Setup & Environment
Language: Python 3.10+

Dependency Management: Use pip and a requirements.txt file.

Core Dependencies:

# requirements.txt
# Input Perception
speechrecognition
pyaudio # Required by SpeechRecognition for microphone access
mss
pillow
pytesseract

# Orchestration
openai # Or other LLM SDK of choice
python-dotenv

# Action Agents
requests

# UI Overlay
PyQt6

Project Structure:

/dia-assistant
|-- /core
|   |-- __init__.py
|   |-- orchestrator.py  # Main logic brain
|   |-- agent_manager.py # Loads and runs agents
|-- /modules
|   |-- __init__.py
|   |-- audio_listener.py
|   |-- screen_scanner.py
|   |-- /agents
|       |-- __init__.py
|       |-- base_agent.py
|       |-- email_agent.py
|-- /ui
|   |-- __init__.py
|   |-- overlay.py
|-- main.py             # Application entry point
|-- requirements.txt
|-- .env                # For API keys
|-- config.json         # For user settings

Initial Setup:

Create a virtual environment: python -m venv venv

Install dependencies: pip install -r requirements.txt

Install Tesseract OCR engine and ensure its path is accessible by your system.

2. Development Milestones & Tasks
This project is broken into a series of milestones. It's recommended to build and test each one sequentially.

Milestone 1: Input Perception (The Senses)
Objective: Create modules that can listen to the user and see the screen.

Task 1.1: Audio Listener (/modules/audio_listener.py)

Create an AudioListener class.

It should run in a dedicated background thread (threading.Thread) to not block the main application.

Use the speech_recognition library to capture microphone input.

Implement a listen_for_triggers() method that continuously listens. Upon detecting speech, it transcribes it and places the text into a thread-safe queue (queue.Queue).

Task 1.2: Screen Scanner (/modules/screen_scanner.py)

Create a ScreenScanner class.

Implement a capture_and_ocr() method.

Use the mss library to take a screenshot of the primary display.

Use Pillow to convert the raw screenshot data into an image format that pytesseract can process.

Use pytesseract.image_to_string() to extract text.

This method should be callable on-demand by the orchestrator.

Milestone 2: Orchestration (The Brain)
Objective: Create the core logic that interprets inputs and decides on an action.

Task 2.1: Orchestrator Core (/core/orchestrator.py)

Create an Orchestrator class.

It will take the AudioListener's output queue as input.

In its main loop, it checks the queue for new transcribed text.

When text is received, it triggers the ScreenScanner to get the current screen context.

Task 2.2: LLM Integration (/core/orchestrator.py)

Create a private method _get_intent_from_llm(speech_text, screen_text).

This method will load an API key from a .env file using python-dotenv.

It will format a prompt for an LLM (e.g., GPT-4, Gemini). The prompt should instruct the model to return a JSON object with intent and entities.

Example Prompt:

{
  "role": "system",
  "content": "You are a helpful AI assistant orchestrator. Based on the user's speech and the text on their screen, determine their intent and extract relevant entities. Respond ONLY with a valid JSON object in the format: {\"intent\": \"intent_name\", \"entities\": {...}, \"confidence\": 0.9}. If no clear intent is found, respond with {\"intent\": \"NONE\"}."
},
{
  "role": "user",
  "content": "Transcribed Speech: \"I'll send that email to Sarah about the Phoenix Project.\"\n\nScreen Text: \"...meeting notes... attendees: Sarah Miller (s.miller@example.com)... Action Items: Finalize Phoenix Project budget...\""
}

The method should parse the LLM's JSON response. If the intent is not NONE, it will pass the intent and entities to the AgentManager.

Milestone 3: Action Agent (The Hands)
Objective: Build the first agent to perform a web-based task.

Task 3.1: Agent Framework (/modules/agents/base_agent.py)

Create an abstract base class BaseAgent.

Define a method signature execute(self, entities: dict).

Task 3.2: Browserbase Email Agent (/modules/agents/email_agent.py)

Create a class BrowserbaseEmailAgent that inherits from BaseAgent.

The execute method will receive entities like recipient_email, subject, etc.

It will use the requests library to interact with the Browserbase API.

Logic:

Read the BROWSERBASE_API_KEY from environment variables.

Make a POST request to the Browserbase API to create a new browser session, navigating to https://mail.google.com.

Use Browserbase's functions (via API calls) to find and click the "Compose" button.

Use Browserbase functions to populate the 'To', 'Subject', and 'Body' fields with the data from the entities dictionary.

Task 3.3: Agent Manager (/core/agent_manager.py)

Create an AgentManager class.

It will have a dictionary mapping intent names to agent classes (e.g., {"SEND_EMAIL": BrowserbaseEmailAgent}).

The Orchestrator will call a method on the manager, like dispatch("SEND_EMAIL", entities), which will then instantiate and run the correct agent.

Milestone 4: Transparent UI & Integration
Objective: Create the user-facing overlay and tie all modules together.

Task 4.1: UI Overlay Window (/ui/overlay.py)

Create an OverlayWindow class inheriting from PyQt6.QtWidgets.QMainWindow.

Set window flags to be frameless and always on top: Qt.WindowType.FramelessWindowHint | Qt.WindowType.WindowStaysOnTopHint.

Make the window background semi-transparent.

Add a QLabel to display the system status.

Implement a method update_status(text: str) that sets the QLabel's text.

Task 4.2: Main Application (main.py)

This is the entry point that initializes everything.

Initialize the PyQt6 application object.

Instantiate the OverlayWindow, AudioListener, ScreenScanner, and Orchestrator.

Crucially, use Qt's signals and slots mechanism for thread-safe communication.

The Orchestrator should have a signal, e.g., status_changed = pyqtSignal(str).

In main.py, connect this signal to the UI's update method: orchestrator.status_changed.connect(overlay_window.update_status).

The orchestrator will emit signals like Listening..., Processing intent..., Drafting email....

Start the AudioListener thread.


Show the UI window and start the Qt event loop.